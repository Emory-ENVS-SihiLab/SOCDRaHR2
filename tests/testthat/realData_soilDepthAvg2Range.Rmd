---
title: "Data from C-PEAT"
author: "C-PEAT and Texas A&M Hackers"
date: "5/8/2018"
output: html_document
---

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

  
```{r setup}
library(tidyverse)
```

```{r eval=FALSE}
library(googledrive)
##pull all the files that we are working with for the hacakthon from the google drive
file.ls <- drive_ls(path='Professional/Data Hackathon TAMU 2018')
cat(sprintf('# %s \n\n', sort(file.ls$name)))
```


# This is a new R script that does the following tasks:
# 1. Takes csv files (given in TAMU Hackathon google drive) as input
# 2. Takes out the column with average soil depth, "soil_cm"" 
# 3. Calculates range, i.e., top and bottom soil depths from the average
# 4. Creates two new columns with top and bottom depth values. Ta-da!

# Data soil depth conversion scripts

## 86-Kvartal.csv 

## Aero.csv 

## Altay.csv 

## Bear.csv 

## Burnt_Village.csv 

```{r Burnt_village}

dataFile <- 'C:/Users/Kritika/Documents/Hackathon/Burnt_Village.csv' #data location on your computer

allData <- read_csv(file=dataFile, skip =1) #big data table of everything!

soilData <- allData[,18] %>% #column number 18 is the soil depth in csv with header "depth_cm"
filter(!is.na(depth_cm) ) #remove empty rows

soilDepthAvg <- soilData$depth_cm

n <- length(soilDepthAvg)

ans <- data.frame(soilDepthAvg,
  layer_top=rep(NA, n),
                  layer_bottom=rep(NA, n))

ans$layer_top[1] <- 0 
ans$layer_bottom[1] <- with(ans, (soilDepthAvg[1]-layer_top[1])*2 + layer_top[1])

for(ii in 2:n){
  ans$layer_top[ii] <- ans$layer_bottom[ii-1]
  ans$layer_bottom[ii] <- with(ans, (soilDepthAvg[ii]-layer_top[ii])*2 + layer_top[ii])
}



```

```
metaData <- allData[,1:2] %>% #first two columns at meta data
filter(!is.na(site_name) ) %>% #remove empty rows
bind_rows(data.frame(site_name='site_name', Burnt_Village='Burnt_Village' )) %>% #transfers miss-identified headers to data table
spread(key=site_name, value = Burnt_Village) #convert long to wide

sampleData01 <- allData[ ,5:14] %>% #identify columns for sample reads
gather(key='header', value='value', -depth, -peat_type) %>% #convert to long format excludes columns that are characters not numerics
filter(!is.na(value) & !grepl('^\\s*$', value)) %>% #remove missing values
mutate(value=as.numeric(value)) #make sure values are numerics

sampleData02 <- allData[ ,17:23] %>% #identify columns for age
rename(header='date_type', value='uncal_date_BP', depth='depth_cm') %>% #harmonize header names to match previous samples
filter(!is.na(value) & !grepl('^\\s*$', value))

sampleData <- sampleData01
bind_rows(sampleData02) #combine this with previous sample


```

## Covey_Hill.csv 

## D127.csv 

## E110.csv 

## Ennadai.csv 

## Glen_Carron.csv 

## Glen_Torridon.csv 

## Goldeye.csv 

## HL02.csv 

## Hongyuan.csv 

## Horse_Trail.csv 

## JBL1.csv 

## JBL2.csv 

## JBL3.csv 

## JBL4.csv 

## JBL5.csv 

## JBL7.csv 

## JBL8.csv 

## Joey.csv 

## KAM12-C1.csv 

## KAM12-C4.csv 

## Kenai_Gasfield.csv 

## KJ2-3.csv 

## KUJU.csv 

## La_Grande2.csv 

## La_Grande3.csv 

## Lac_Le_Caron.csv 

## Lake396.csv 

## Lake785.csv 

## Lebel.csv 

## Lompolojankka.csv 

## Mariana.csv 

## Martin.csv 

## Mosaik.csv 

## No_Name_Creek.csv 

## Nuikluk.csv 

## NW-BG.csv 

## Ours.csv 

## Patuanak.csv 

## Petersville.csv 

## Petite_Bog.csv 

## Plaine.csv 

## Rogovaya.csv 

## Saarisuo.csv 

## Selwyn.csv 

## Shuttle.csv 

## SIB06.csv 

## Sidney.csv 

## Siikaneva.csv 

## Slave.csv 

## Sterne.csv 

## Stordalen.csv 

## Sundance.csv 

## Swanson.csv 

## T1.csv 

## Unit.csv 

## Upper_Pinto.csv 

## Usinsk.csv 

## Utikuma.csv 

## V34.csv 

## Vasyugan.csv 

## VC04-06.csv 

## Zoige.csv